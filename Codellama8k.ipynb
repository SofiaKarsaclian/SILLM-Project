{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2_kyUpic3Jj",
        "outputId": "dbbb26a3-c1e5-4e21-e4b2-e55ec45c2c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Collecting auto-gptq\n",
            "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.26.0 (from auto-gptq)\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from auto-gptq)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.25.2)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.0.7-py3-none-any.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.2.1+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.40.0.dev0)\n",
            "Collecting peft>=0.5.0 (from auto-gptq)\n",
            "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->auto-gptq)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Collecting xxhash (from datasets->auto-gptq)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Installing collected packages: xxhash, rouge, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gekko, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, accelerate, peft, auto-gptq\n",
            "Successfully installed accelerate-0.28.0 auto-gptq-0.7.1+cu118 datasets-2.18.0 dill-0.3.8 gekko-1.0.7 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 peft-0.10.0 rouge-1.0.1 xxhash-3.4.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install -q -U git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip3 install -q -U git+https://github.com/huggingface/optimum.git\n",
        "\n",
        "!pip3 install -q -U flash-attn\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u7sfyTF2dM_E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sofik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
        "import torch\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q8nSi5HdKlv",
        "outputId": "a57265ac-b6c2-4fe3-d7be-c7ee92b9a64a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_id = \"TheBloke/CodeLlama-7B-Instruct-GPTQ\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "          bnb_4bit_use_double_quant=True,\n",
        "          bnb_4bit_quant_type=\"nf4\",\n",
        "          bnb_4bit_compute_dtype=torch.bfloat16\n",
        " )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    revision = \"main\",\n",
        "    trust_remote_code = True,\n",
        "    cache_dir = ''\n",
        "    )\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Oyfr5vpYdHFR"
      },
      "outputs": [],
      "source": [
        "# Use prompts created for the zero-shot task\n",
        "with open(\"/content/zero_prompts.json\", \"r\") as file:\n",
        "    zero_prompts = json.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W9U6RQE5dEst"
      },
      "outputs": [],
      "source": [
        "# Delimiters\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "system_prompt=\"\"\"You are a helpful assistant. You provide accurate information.\"\"\"\n",
        "\n",
        "# Define a stream *without* function calling capabilities\n",
        "def stream(user_prompt):\n",
        "    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").input_ids.cuda()\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
        "        generated_text = model.generate(inputs=inputs, streamer=streamer, max_new_tokens=500, temperature=0.01, do_sample=True)\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77L1uTrplPtd",
        "outputId": "19075869-2540-43b7-d3a5-61b0018adb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7900 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: There has been some discussion about abortion during recent\n",
            "years. Which one of the opinions on this page best agrees with\n",
            "your view?\n",
            "1. By law, abortion should never be permitted\n",
            "2. The law should permit abortion only in case of rape, incest, or\n",
            "when the woman's life is in danger\n",
            "3. The law should permit abortion other than for\n",
            "rape/incest/danger to woman but only after need clearly\n",
            "established\n",
            "4. By law, a woman should always be able to obtain an abortion\n",
            "as a matter of personal choice [/INST]  Here is the estimated answer distribution for the question \"Which one of the opinions on this page best agrees with your view?\" in 2020:\n",
            "\n",
            "{1: 10.5%, 2: 20.5%, 3: 48.5%, 4: 28.5%}\n",
            "\n",
            "This means that 10.5% of respondents would have answered that abortion should never be permitted by law, 20.5% would have answered that the law should permit abortion only in case of rape, incest, or when the woman's life is in danger, 48.5% would have answered that the law should permit abortion other than for rape/incest/danger to woman but only after need clearly established, and 28.5% would have answered that by law, a woman should always be able to obtain an abortion as a matter of personal choice.</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 8208 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Would you be pleased, upset, or neither pleased nor upset if the\n",
            "Supreme Court reduced abortion rights?\n",
            "1. Extremely pleased\n",
            "2. Moderately pleased\n",
            "3. A little pleased\n",
            "4. Neither pleased nor upset\n",
            "5. A little upset\n",
            "6. Moderately upset\n",
            "7. Extremely upset [/INST]  {\n",
            "1: 30.4%\n",
            "2: 21.2%\n",
            "3: 12.3%\n",
            "4: 36.3%\n",
            "5: 10.1%\n",
            "6: 4.7%\n",
            "7: 0.9%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7381 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: How much, if at all, do you think climate change is currently\n",
            "affecting severe weather events or temperature patterns in the\n",
            "United States?\n",
            "1. Not at all\n",
            "2. A little\n",
            "3. A moderate amount\n",
            "4. A lot\n",
            "5. A great deal [/INST]  {\n",
            "1: 10.5%,\n",
            "2: 22.5%,\n",
            "3: 32.5%,\n",
            "4: 15.5%,\n",
            "5: 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7383 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: How important is the issue of climate change to you personally?\n",
            "1. Not at all important\n",
            "2. A little important\n",
            "3. Moderately important\n",
            "4. Very important\n",
            "5. Extremely important [/INST]  {\n",
            "1: 20.5%,\n",
            "2: 22.5%,\n",
            "3: 31.5%,\n",
            "4: 15.5%,\n",
            "5: 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7375 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you think the federal government should make it more\n",
            "difficult for people to buy a gun than it is now, make it easier for\n",
            "people to buy a gun, or keep these rules about the same as they\n",
            "are now?\n",
            "1. More difficult\n",
            "2. Easier\n",
            "3. Keep these rules about the same [/INST]  {\n",
            "1: 30.5%,\n",
            "2: 27.5%,\n",
            "3: 31.5%\n",
            "}\n",
            "\n",
            "This is based on a representative sample of 7375 US-Americans in 2020. The results are based on a survey conducted by the Pew Research Center.</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7387 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor, oppose, or neither favor nor oppose requiring\n",
            "background checks for gun purchases at gun shows or other\n",
            "private sales?\n",
            "1. Favor a great deal\n",
            "2. Favor a moderate amount\n",
            "3. Favor a little\n",
            "4. Neither favor nor oppose\n",
            "5. Oppose a little\n",
            "6. Oppose a moderate amount\n",
            "7. Oppose a great deal [/INST]  {\n",
            "\"Favor a great deal\": 10.5%,\n",
            "\"Favor a moderate amount\": 22.5%,\n",
            "\"Favor a little\": 25.5%,\n",
            "\"Neither favor nor oppose\": 42.5%,\n",
            "\"Oppose a little\": 15.5%,\n",
            "\"Oppose a moderate amount\": 17.5%,\n",
            "\"Oppose a great deal\": 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7381 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor, oppose, or neither favor nor oppose banning the\n",
            "sale of semi-automatic “assault-style” rifles?\n",
            "1. Favor a great deal\n",
            "2. Favor a moderate amount\n",
            "3. Favor a little\n",
            "4. Neither favor nor oppose\n",
            "5. Oppose a little\n",
            "6. Oppose a moderate amount\n",
            "7. Oppose a great deal [/INST]  {\n",
            "\"Favor a great deal\": 10.5%,\n",
            "\"Favor a moderate amount\": 22.5%,\n",
            "\"Favor a little\": 25.5%,\n",
            "\"Neither favor nor oppose\": 42.5%,\n",
            "\"Oppose a little\": 15.5%,\n",
            "\"Oppose a moderate amount\": 17.5%,\n",
            "\"Oppose a great deal\": 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7350 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you think the number of immigrants from foreign countries\n",
            "who are permitted to come to the United States to live should be\n",
            "increased a lot, increased a little, left the same as it is now,\n",
            "decreased a little, or decreased a lot?\n",
            "1. Increased a lot\n",
            "2. Increased a little\n",
            "3. Left the same as it is now\n",
            "4. Decreased a little\n",
            "5. Decreased a lot [/INST]  {\n",
            "1: 30.5%\n",
            "2: 22.5%\n",
            "3: 25.5%\n",
            "4: 10.5%\n",
            "5: 5.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7389 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Now I’d like to ask you about immigration in recent years.\n",
            "How likely is it that recent immigration levels will take jobs away\n",
            "from people already here – extremely likely, very likely,\n",
            "somewhat likely, or not at all likely?\n",
            "1. Extremely likely\n",
            "2. Very likely\n",
            "3. Somewhat likely\n",
            "4. Not at all likely [/INST]  {\n",
            "1: 30.5%\n",
            "2: 27.5%\n",
            "3: 21.5%\n",
            "4: 18.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7397 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor, oppose, or neither favor nor oppose allowing\n",
            "refugees who are fleeing war, persecution, or natural disasters\n",
            "in other countries to come to live in the U.S.?\n",
            "1. Favor a great deal\n",
            "2. Favor a moderate amount\n",
            "3. Favor a little\n",
            "4. Neither favor nor oppose\n",
            "5. Oppose a little\n",
            "6. Oppose a moderate amount\n",
            "7. Oppose a great deal [/INST]  Here is the estimated answer distribution for the question \"Do you favor, oppose, or neither favor nor oppose allowing refugees who are fleeing war, persecution, or natural disasters in other countries to come to live in the U.S.?\" based on a representative sample of 7397 US-Americans in 2020:\n",
            "\n",
            "{\n",
            "\"1. Favor a great deal\": 10.5%,\n",
            "\"2. Favor a moderate amount\": 22.5%,\n",
            "\"3. Favor a little\": 27.5%,\n",
            "\"4. Neither favor nor oppose\": 22.5%,\n",
            "\"5. Oppose a little\": 10.5%,\n",
            "\"6. Oppose a moderate amount\": 12.5%,\n",
            "\"7. Oppose a great deal\": 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7322 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor an increase, decrease, or no change in\n",
            "government spending tohelp people pay for health insurance\n",
            "when they can’t pay for it all themselves?\n",
            "1. Increase a great deal\n",
            "2. Increase a moderate amount\n",
            "3. Increase a little\n",
            "4. No change\n",
            "5. Decrease a little\n",
            "6. Decrease a moderate amount\n",
            "7. Decrease a great deal [/INST]  {\n",
            "\"1\": 23.3%,\n",
            "\"2\": 22.2%,\n",
            "\"3\": 19.9%,\n",
            "\"4\": 22.2%,\n",
            "\"5\": 19.9%,\n",
            "\"6\": 12.3%,\n",
            "\"7\": 10.1%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7900 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: There has been some discussion about abortion during recent\n",
            "years. Which one of the opinions on this page best agrees with\n",
            "your view?\n",
            "1. By law, abortion should never be permitted\n",
            "2. The law should permit abortion only in case of rape, incest, or\n",
            "when the woman's life is in danger\n",
            "3. The law should permit abortion other than for\n",
            "rape/incest/danger to woman but only after need clearly\n",
            "established\n",
            "4. By law, a woman should always be able to obtain an abortion\n",
            "as a matter of personal choice [/INST]  Here is the estimated answer distribution for the question \"Which one of the opinions on this page best agrees with your view?\" based on a representative sample of 7900 US-Americans in 2020:\n",
            "\n",
            "{\n",
            "1: 10.3%,\n",
            "2: 21.5%,\n",
            "3: 32.8%,\n",
            "4: 26.9%\n",
            "}\n",
            "\n",
            "This means that 10.3% of respondents believed that abortion should never be permitted by law, 21.5% believed that the law should permit abortion only in cases of rape, incest, or when the woman's life is in danger, 32.8% believed that the law should permit abortion other than for rape/incest/danger to woman but only after need clearly established, and 26.9% believed that a woman should always be able to obtain an abortion as a matter of personal choice.</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 8208 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Would you be pleased, upset, or neither pleased nor upset if the\n",
            "Supreme Court reduced abortion rights?\n",
            "1. Extremely pleased\n",
            "2. Moderately pleased\n",
            "3. A little pleased\n",
            "4. Neither pleased nor upset\n",
            "5. A little upset\n",
            "6. Moderately upset\n",
            "7. Extremely upset [/INST]  {\n",
            "1: 30.5%\n",
            "2: 21.5%\n",
            "3: 12.5%\n",
            "4: 26.5%\n",
            "5: 10.5%\n",
            "6: 5.5%\n",
            "7: 0.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7381 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: How much, if at all, do you think climate change is currently\n",
            "affecting severe weather events or temperature patterns in the\n",
            "United States?\n",
            "1. Not at all\n",
            "2. A little\n",
            "3. A moderate amount\n",
            "4. A lot\n",
            "5. A great deal [/INST]  {\n",
            "1: 10.5%,\n",
            "2: 22.5%,\n",
            "3: 32.5%,\n",
            "4: 15.5%,\n",
            "5: 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7383 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: How important is the issue of climate change to you personally?\n",
            "1. Not at all important\n",
            "2. A little important\n",
            "3. Moderately important\n",
            "4. Very important\n",
            "5. Extremely important [/INST]  {\n",
            "1: 20.5%,\n",
            "2: 22.5%,\n",
            "3: 31.5%,\n",
            "4: 15.5%,\n",
            "5: 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7375 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you think the federal government should make it more\n",
            "difficult for people to buy a gun than it is now, make it easier for\n",
            "people to buy a gun, or keep these rules about the same as they\n",
            "are now?\n",
            "1. More difficult\n",
            "2. Easier\n",
            "3. Keep these rules about the same [/INST]  {\n",
            "1: 30.5%,\n",
            "2: 27.5%,\n",
            "3: 31.5%\n",
            "}\n",
            "\n",
            "This is based on a representative sample of 7375 US-Americans in 2020. The results are based on a survey conducted by the Pew Research Center.</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7387 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor, oppose, or neither favor nor oppose requiring\n",
            "background checks for gun purchases at gun shows or other\n",
            "private sales?\n",
            "1. Favor a great deal\n",
            "2. Favor a moderate amount\n",
            "3. Favor a little\n",
            "4. Neither favor nor oppose\n",
            "5. Oppose a little\n",
            "6. Oppose a moderate amount\n",
            "7. Oppose a great deal [/INST]  {\n",
            "\"Favor a great deal\": 10.5%,\n",
            "\"Favor a moderate amount\": 22.5%,\n",
            "\"Favor a little\": 25.5%,\n",
            "\"Neither favor nor oppose\": 42.5%,\n",
            "\"Oppose a little\": 15.5%,\n",
            "\"Oppose a moderate amount\": 17.5%,\n",
            "\"Oppose a great deal\": 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7381 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor, oppose, or neither favor nor oppose banning the\n",
            "sale of semi-automatic “assault-style” rifles?\n",
            "1. Favor a great deal\n",
            "2. Favor a moderate amount\n",
            "3. Favor a little\n",
            "4. Neither favor nor oppose\n",
            "5. Oppose a little\n",
            "6. Oppose a moderate amount\n",
            "7. Oppose a great deal [/INST]  {\n",
            "\"Favor a great deal\": 10.5%,\n",
            "\"Favor a moderate amount\": 22.5%,\n",
            "\"Favor a little\": 25.5%,\n",
            "\"Neither favor nor oppose\": 42.5%,\n",
            "\"Oppose a little\": 15.5%,\n",
            "\"Oppose a moderate amount\": 17.5%,\n",
            "\"Oppose a great deal\": 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7350 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you think the number of immigrants from foreign countries\n",
            "who are permitted to come to the United States to live should be\n",
            "increased a lot, increased a little, left the same as it is now,\n",
            "decreased a little, or decreased a lot?\n",
            "1. Increased a lot\n",
            "2. Increased a little\n",
            "3. Left the same as it is now\n",
            "4. Decreased a little\n",
            "5. Decreased a lot [/INST]  {\n",
            "1: 30.5%\n",
            "2: 21.5%\n",
            "3: 27.5%\n",
            "4: 10.5%\n",
            "5: 5.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7389 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Now I’d like to ask you about immigration in recent years.\n",
            "How likely is it that recent immigration levels will take jobs away\n",
            "from people already here – extremely likely, very likely,\n",
            "somewhat likely, or not at all likely?\n",
            "1. Extremely likely\n",
            "2. Very likely\n",
            "3. Somewhat likely\n",
            "4. Not at all likely [/INST]  {\n",
            "1: 30.5%\n",
            "2: 27.5%\n",
            "3: 21.5%\n",
            "4: 18.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7397 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor, oppose, or neither favor nor oppose allowing\n",
            "refugees who are fleeing war, persecution, or natural disasters\n",
            "in other countries to come to live in the U.S.?\n",
            "1. Favor a great deal\n",
            "2. Favor a moderate amount\n",
            "3. Favor a little\n",
            "4. Neither favor nor oppose\n",
            "5. Oppose a little\n",
            "6. Oppose a moderate amount\n",
            "7. Oppose a great deal [/INST]  Here is the estimated answer distribution for the question \"Do you favor, oppose, or neither favor nor oppose allowing refugees who are fleeing war, persecution, or natural disasters in other countries to come to live in the U.S.?\" based on a representative sample of 7397 US-Americans in 2020:\n",
            "\n",
            "{\n",
            "\"1. Favor a great deal\": 10.5%,\n",
            "\"2. Favor a moderate amount\": 22.5%,\n",
            "\"3. Favor a little\": 27.5%,\n",
            "\"4. Neither favor nor oppose\": 22.5%,\n",
            "\"5. Oppose a little\": 10.5%,\n",
            "\"6. Oppose a moderate amount\": 12.5%,\n",
            "\"7. Oppose a great deal\": 10.5%\n",
            "}</s>\n",
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant. You provide accurate information.\n",
            "<</SYS>>\n",
            "\n",
            "In the following you will be given a question followed several possible answer categories. Please     estimate how a representative sample of 7322 US-Americans would have answered the question in 2020 by providing the    number of respondents for each answer option in percent (output a dictionary object with answer category number as    key and answer frequency in percent as value. Only provide the number without the percentage sign).    Here is the questions: Do you favor an increase, decrease, or no change in\n",
            "government spending tohelp people pay for health insurance\n",
            "when they can’t pay for it all themselves?\n",
            "1. Increase a great deal\n",
            "2. Increase a moderate amount\n",
            "3. Increase a little\n",
            "4. No change\n",
            "5. Decrease a little\n",
            "6. Decrease a moderate amount\n",
            "7. Decrease a great deal [/INST]  {\n",
            "\"1\": 23.3%,\n",
            "\"2\": 22.2%,\n",
            "\"3\": 19.9%,\n",
            "\"4\": 22.2%,\n",
            "\"5\": 19.9%,\n",
            "\"6\": 12.3%,\n",
            "\"7\": 10.1%\n",
            "}</s>\n"
          ]
        }
      ],
      "source": [
        "cllama_ans = {}\n",
        "# for question in zero_prompts:\n",
        "#   answer= stream(zero_prompts[question])\n",
        "#   answer_clean= re.findall(r'\\{([^{}]*)\\}', answer)\n",
        "#   cllama_ans[question] = answer_clean\n",
        "#   torch.cuda.empty_cache()\n",
        "\n",
        "# two runs\n",
        "for i in range(2):\n",
        "    for question in zero_prompts:\n",
        "        answer = stream(zero_prompts[question])\n",
        "        answer_clean = re.findall(r'\\{([^{}]*)\\}', answer) # extract dict from answer\n",
        "        answer_dict = [eval(i) for i in answer_clean]\n",
        "\n",
        "        if question not in cllama_ans:\n",
        "            cllama_ans[question] = []\n",
        "        cllama_ans[question].append(answer_dict)\n",
        "\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TeNbu-3_pimP"
      },
      "outputs": [],
      "source": [
        "cllama_ans\n",
        "\n",
        "cllama_answers = json.dumps(cllama_ans)\n",
        "with open(\"cllama_answers.json\", \"w\") as file:\n",
        "    file.write(cllama_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"cllama_answers.json\", \"r\") as file:\n",
        "    cllama_answers = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'V201336': [['1: 10.5%, 2: 20.5%, 3: 48.5%, 4: 28.5%'],\n",
              "  ['\\n1: 10.3%,\\n2: 21.5%,\\n3: 32.8%,\\n4: 26.9%\\n']],\n",
              " 'V201342x': [['\\n1: 30.4%\\n2: 21.2%\\n3: 12.3%\\n4: 36.3%\\n5: 10.1%\\n6: 4.7%\\n7: 0.9%\\n'],\n",
              "  ['\\n1: 30.5%\\n2: 21.5%\\n3: 12.5%\\n4: 26.5%\\n5: 10.5%\\n6: 5.5%\\n7: 0.5%\\n']],\n",
              " 'V202332': [['\\n1: 10.5%,\\n2: 22.5%,\\n3: 32.5%,\\n4: 15.5%,\\n5: 10.5%\\n'],\n",
              "  ['\\n1: 10.5%,\\n2: 22.5%,\\n3: 32.5%,\\n4: 15.5%,\\n5: 10.5%\\n']],\n",
              " 'V202333': [['\\n1: 20.5%,\\n2: 22.5%,\\n3: 31.5%,\\n4: 15.5%,\\n5: 10.5%\\n'],\n",
              "  ['\\n1: 20.5%,\\n2: 22.5%,\\n3: 31.5%,\\n4: 15.5%,\\n5: 10.5%\\n']],\n",
              " 'V202337': [['\\n1: 30.5%,\\n2: 27.5%,\\n3: 31.5%\\n'],\n",
              "  ['\\n1: 30.5%,\\n2: 27.5%,\\n3: 31.5%\\n']],\n",
              " 'V202341x': [['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n'],\n",
              "  ['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n']],\n",
              " 'V202344x': [['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n'],\n",
              "  ['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n']],\n",
              " 'V202232': [['\\n1: 30.5%\\n2: 22.5%\\n3: 25.5%\\n4: 10.5%\\n5: 5.5%\\n'],\n",
              "  ['\\n1: 30.5%\\n2: 21.5%\\n3: 27.5%\\n4: 10.5%\\n5: 5.5%\\n']],\n",
              " 'V202233': [['\\n1: 30.5%\\n2: 27.5%\\n3: 21.5%\\n4: 18.5%\\n'],\n",
              "  ['\\n1: 30.5%\\n2: 27.5%\\n3: 21.5%\\n4: 18.5%\\n']],\n",
              " 'V202236x': [['\\n\"1. Favor a great deal\": 10.5%,\\n\"2. Favor a moderate amount\": 22.5%,\\n\"3. Favor a little\": 27.5%,\\n\"4. Neither favor nor oppose\": 22.5%,\\n\"5. Oppose a little\": 10.5%,\\n\"6. Oppose a moderate amount\": 12.5%,\\n\"7. Oppose a great deal\": 10.5%\\n'],\n",
              "  ['\\n\"1. Favor a great deal\": 10.5%,\\n\"2. Favor a moderate amount\": 22.5%,\\n\"3. Favor a little\": 27.5%,\\n\"4. Neither favor nor oppose\": 22.5%,\\n\"5. Oppose a little\": 10.5%,\\n\"6. Oppose a moderate amount\": 12.5%,\\n\"7. Oppose a great deal\": 10.5%\\n']],\n",
              " 'V202380x': [['\\n\"1\": 23.3%,\\n\"2\": 22.2%,\\n\"3\": 19.9%,\\n\"4\": 22.2%,\\n\"5\": 19.9%,\\n\"6\": 12.3%,\\n\"7\": 10.1%\\n'],\n",
              "  ['\\n\"1\": 23.3%,\\n\"2\": 22.2%,\\n\"3\": 19.9%,\\n\"4\": 22.2%,\\n\"5\": 19.9%,\\n\"6\": 12.3%,\\n\"7\": 10.1%\\n']]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1: 10.5%, 2: 20.5%, 3: 48.5%, 4: 28.5%']\n",
            "['\\n1: 10.3%,\\n2: 21.5%,\\n3: 32.8%,\\n4: 26.9%\\n']\n",
            "['\\n1: 30.4%\\n2: 21.2%\\n3: 12.3%\\n4: 36.3%\\n5: 10.1%\\n6: 4.7%\\n7: 0.9%\\n']\n",
            "['\\n1: 30.5%\\n2: 21.5%\\n3: 12.5%\\n4: 26.5%\\n5: 10.5%\\n6: 5.5%\\n7: 0.5%\\n']\n",
            "['\\n1: 10.5%,\\n2: 22.5%,\\n3: 32.5%,\\n4: 15.5%,\\n5: 10.5%\\n']\n",
            "['\\n1: 10.5%,\\n2: 22.5%,\\n3: 32.5%,\\n4: 15.5%,\\n5: 10.5%\\n']\n",
            "['\\n1: 20.5%,\\n2: 22.5%,\\n3: 31.5%,\\n4: 15.5%,\\n5: 10.5%\\n']\n",
            "['\\n1: 20.5%,\\n2: 22.5%,\\n3: 31.5%,\\n4: 15.5%,\\n5: 10.5%\\n']\n",
            "['\\n1: 30.5%,\\n2: 27.5%,\\n3: 31.5%\\n']\n",
            "['\\n1: 30.5%,\\n2: 27.5%,\\n3: 31.5%\\n']\n",
            "['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n']\n",
            "['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n']\n",
            "['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n']\n",
            "['\\n\"Favor a great deal\": 10.5%,\\n\"Favor a moderate amount\": 22.5%,\\n\"Favor a little\": 25.5%,\\n\"Neither favor nor oppose\": 42.5%,\\n\"Oppose a little\": 15.5%,\\n\"Oppose a moderate amount\": 17.5%,\\n\"Oppose a great deal\": 10.5%\\n']\n",
            "['\\n1: 30.5%\\n2: 22.5%\\n3: 25.5%\\n4: 10.5%\\n5: 5.5%\\n']\n",
            "['\\n1: 30.5%\\n2: 21.5%\\n3: 27.5%\\n4: 10.5%\\n5: 5.5%\\n']\n",
            "['\\n1: 30.5%\\n2: 27.5%\\n3: 21.5%\\n4: 18.5%\\n']\n",
            "['\\n1: 30.5%\\n2: 27.5%\\n3: 21.5%\\n4: 18.5%\\n']\n",
            "['\\n\"1. Favor a great deal\": 10.5%,\\n\"2. Favor a moderate amount\": 22.5%,\\n\"3. Favor a little\": 27.5%,\\n\"4. Neither favor nor oppose\": 22.5%,\\n\"5. Oppose a little\": 10.5%,\\n\"6. Oppose a moderate amount\": 12.5%,\\n\"7. Oppose a great deal\": 10.5%\\n']\n",
            "['\\n\"1. Favor a great deal\": 10.5%,\\n\"2. Favor a moderate amount\": 22.5%,\\n\"3. Favor a little\": 27.5%,\\n\"4. Neither favor nor oppose\": 22.5%,\\n\"5. Oppose a little\": 10.5%,\\n\"6. Oppose a moderate amount\": 12.5%,\\n\"7. Oppose a great deal\": 10.5%\\n']\n",
            "['\\n\"1\": 23.3%,\\n\"2\": 22.2%,\\n\"3\": 19.9%,\\n\"4\": 22.2%,\\n\"5\": 19.9%,\\n\"6\": 12.3%,\\n\"7\": 10.1%\\n']\n",
            "['\\n\"1\": 23.3%,\\n\"2\": 22.2%,\\n\"3\": 19.9%,\\n\"4\": 22.2%,\\n\"5\": 19.9%,\\n\"6\": 12.3%,\\n\"7\": 10.1%\\n']\n"
          ]
        }
      ],
      "source": [
        "for question, answers in cllama_answers.items():\n",
        "    processed_answers = []\n",
        "    for answer in answers:\n",
        "        print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "map_answers = {\n",
        "    \"Favor a great deal\": 1,\n",
        "    \"Favor a moderate amount\": 2,\n",
        "    \"Favor a little\": 3,\n",
        "    \"Neither favor nor oppose\": 4,\n",
        "    \"Oppose a little\": 5,\n",
        "    \"Oppose a moderate amount\": 6,\n",
        "    \"Oppose a great deal\": 7\n",
        "}\n",
        "\n",
        "processed_cllama_ans = {}\n",
        "\n",
        "for question, answers in cllama_answers.items():\n",
        "    processed_answers = []\n",
        "    for answer in answers:\n",
        "        clean_answer = re.sub(r'\"[^\"]*\"', '', answer[0])\n",
        "        clean_answer = clean_answer.replace('%', ', ')\n",
        "        # Map textual answer to its corresponding numeric value\n",
        "        # if clean_answer.strip() in map_answers:\n",
        "        #     clean_answer = map_answers[clean_answer.strip()]\n",
        "\n",
        "        # Replace '\\n' with '' if it's at the beginning or following a comma\n",
        "        clean_answer = re.sub(r'(?:^|,)\\s*\\n', '', clean_answer)\n",
        "        # Map textual answer to its corresponding numeric value\n",
        "\n",
        "        processed_answers.append(clean_answer)\n",
        "    processed_cllama_ans[question] = processed_answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_cllama_ans\n",
        "# ended up editing them manually\n",
        "\n",
        "with open('cllama_ans_clean.txt', 'r') as file:\n",
        "    # Read the content of the file\n",
        "    cllama_ans = file.readlines()\n",
        "\n",
        "cllama_ans = [line.strip() for line in cllama_ans]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"{'V201336': ['1: 10.5, , 2: 20.5, , 3: 48.5, , 4: 28.5', '1: 10.3, 2: 21.5, 3: 32.8, 4: 26.9'],\",\n",
              " \"'V201342x': ['1: 30.4, 2: 21.2, 3: 12.3, 4: 36.3, 5: 10.1, 6: 4.7, 7: 0.9', '1: 30.5, 2: 21.5, 3: 12.5, 4: 26.5, 5: 10.5, 6: 5.5, 7: 0.5'],\",\n",
              " \"'V202332': ['1: 10.5, 2: 22.5, 3: 32.5, 4: 15.5, 5: 10.5','1: 10.5, 2: 22.5, 3: 32.5, 4: 15.5, 5: 10.5'],\",\n",
              " \"'V202333': ['1: 20.5, 2: 22.5, 3: 31.5, 4: 15.5, 5: 10.5','1: 20.5, 2: 22.5, 3: 31.5, 4: 15.5, 5: 10.5'],\",\n",
              " \"'V202337': ['1: 30.5, 2: 27.5, 3: 31.5', '1: 30.5, 2: 27.5, 3: 31.5'],\",\n",
              " \"'V202341x': ['1: 10.5, 2: 22.5, 3: 25.5, 4: 42.5, 5: 15.5, 6: 17.5, 7: 10.5','1: 10.5, 2: 22.5, 3: 25.5, 4: 42.5, 5: 15.5, 6: 17.5, 7: 10.5'],\",\n",
              " \"'V202344x': ['1: 10.5, 2: 22.5, 3: 25.5, 4: 42.5, 5: 15.5, 6: 17.5, 7: 10.5','1: 10.5, 2: 22.5, 3: 25.5, 4: 42.5, 5: 15.5, 6: 17.5, 7: 10.5'],\",\n",
              " \"'V202232': ['1: 30.5, 2: 22.5, 3: 25.5, 4: 10.5, 5: 5.5','1: 30.5, 2: 21.5, 3: 27.5, 4: 10.5, 5: 5.5'],\",\n",
              " \"'V202233': ['1: 30.5, 2: 27.5, 3: 21.5, 4: 18.5', '1: 30.5, 2: 27.5, 3: 21.5, 4: 18.5'],\",\n",
              " \"'V202236x': ['1: 10.5, 2: 22.5, 3: 27.5, 4: 22.5, 5: 10.5, 6: 12.5, 7: 10.5', '1: 10.5, 2: 22.5, 3: 27.5, 4: 22.5, 5: 10.5, 6: 12.5, 7: 10.5'],\",\n",
              " \"'V202380x': ['1: 23.3, 2: 22.2, 3: 19.9, 4: 22.2, 5: 19.9, 6: 12.3, 7: 10.1', '1: 23.3, 2: 22.2, 3: 19.9, 4: 22.2, 5: 19.9, 6: 12.3, 7: 10.1']}\"]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cllama_ans"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
